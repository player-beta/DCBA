{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from model.classify_models import alexnet\n",
    "from utils import LabeledDataset, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_trans = transforms.Compose([\n",
    "\ttransforms.Resize((32, 32)),   # resize 参数是元组\n",
    "\ttransforms.ToTensor()\n",
    "])\n",
    "cifar_trans = transforms.Compose([\n",
    "\ttransforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model = alexnet(\"cifar10\",False)\n",
    "model.load_state_dict(torch.load(\"/home/mhc/AIJack/invert_and_poison/checkpoint/experiment_50/globmod/epoch_24_acc_0.715.pth\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(lr, iter):\n",
    "\t\"\"\"Sets the learning rate to the initial LR decayed by 0.5 every 1000 iterations\"\"\"\n",
    "\tlr = lr * (0.8 ** (iter // 1000))\n",
    "\treturn lr\n",
    "\n",
    "def save_img(img_tensor, fname):\n",
    "\ttoPIL = transforms.ToPILImage()\n",
    "\timg = toPIL(img_tensor)\n",
    "\timg.save(fname, quality=95, subsampling=0)\n",
    "\n",
    "class AverageMeter(object):\n",
    "\t\"\"\"Computes and stores the average and current value\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.reset()\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.val = 0\n",
    "\t\tself.avg = 0\n",
    "\t\tself.sum = 0\n",
    "\t\tself.count = 0\n",
    "\n",
    "\tdef update(self, val, n=1):\n",
    "\t\tself.val = val\n",
    "\t\tself.sum += val\n",
    "\t\tself.count += n\n",
    "\t\tself.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def custom_dataset(dataset, classlist, start_idx):\n",
    "\tsize = 200\n",
    "\tdatalist = []\n",
    "\tif dataset == 'cifar10':\n",
    "\t\tfor cls in classlist:\n",
    "\t\t\tds = LabeledDataset('cifar10', f\"/home/mhc/public_dataset/cifar_imgs/train/{cls}\", cls, (start_idx+1, start_idx+size+1), cifar_trans)\n",
    "\t\t\tdatalist.append(ds)\n",
    "\tif dataset == 'mnist':\n",
    "\t\tfor cls in classlist:\n",
    "\t\t\tds = LabeledDataset('mnist', f\"/home/mhc/public_dataset/mnist_imgs/train/{cls}\", cls, (start_idx+1, start_idx+size+1), mnist_trans)\n",
    "\t\t\tdatalist.append(ds)\n",
    "\t\n",
    "\tdatatup = tuple(datalist)\n",
    "\tconcat_ds = torch.utils.data.ConcatDataset(datatup)\n",
    "\treturn concat_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd(model, data, trigger):\n",
    "    model.eval() # 固定BN层和Dropout层\n",
    "\n",
    "    num_iter = 3000\n",
    "    lr = 0.01\n",
    "    eps = 1\n",
    "    losses = AverageMeter()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    tri = trigger.clone()\n",
    "    \n",
    "    # tri = nn.Parameter(tri.to(device))\n",
    "    # tri.requires_grad = True\n",
    "\n",
    "    input, label = data\n",
    "    input = input.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    # PGD iteration \n",
    "    for j in range(num_iter):\n",
    "        if tri.grad == None:\n",
    "            tri = nn.Parameter(tri.to(device))\n",
    "            tri.requires_grad=True\n",
    "        \n",
    "        else:\n",
    "            tri.grad.data.zero_()\n",
    "        \n",
    "        lr1 = adjust_learning_rate(lr, j+1)\n",
    "        \n",
    "        input[:, :, 22:30, 22:30] = tri\n",
    "        # 加上trigger的样本的目标标签已经提前设置好\n",
    "        \n",
    "        output, _ = model(input)\n",
    "       \n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        \n",
    "        trigrad = tri.grad\n",
    "        tri = tri - lr1*tri.grad\n",
    "        tri = torch.clamp(tri, 0, eps).detach_()\n",
    "        \n",
    "        if (j+1) %100 == 0:\n",
    "            print(\" iter: {:5d} | LR: {:2.4f} | Loss Val: {}\"\n",
    "\t\t\t\t\t\t.format(j, lr1, loss.item()))\n",
    "                        \n",
    "    return tri\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trigger generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhc/.conda/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iter:    99 | LR: 0.0100 | Loss Val: 3.0270025730133057\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 2.3141930103302\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 2.0421202182769775\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 1.8268547058105469\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 1.6566826105117798\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 1.5233631134033203\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 1.4102604389190674\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 1.3324217796325684\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 1.273990511894226\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 1.2245408296585083\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 1.1924363374710083\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 1.1640163660049438\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 1.1371262073516846\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 1.111333966255188\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 1.0861246585845947\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 1.0629485845565796\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 1.0433136224746704\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 1.022809386253357\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 1.0021069049835205\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.984192967414856\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.9717986583709717\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.9597623944282532\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.9483993649482727\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.9374334812164307\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.9270758032798767\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.9172889590263367\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.9078789353370667\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.8992911577224731\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.8907419443130493\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.8826162219047546\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.8123630285263062\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.7972169518470764\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.7851669788360596\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.7750322222709656\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.766331672668457\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.7569339275360107\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.748022198677063\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.739724338054657\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.731734573841095\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.7237767577171326\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.7181638479232788\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.7131389379501343\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.7085815072059631\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.7040963768959045\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.6994303464889526\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.6952803134918213\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.6912574172019958\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.6873942017555237\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.683758556842804\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.6801576614379883\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.6774539947509766\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.6748806834220886\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.6723254919052124\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.66995769739151\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.6675657629966736\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.6652035117149353\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.6628024578094482\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.6605144739151001\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.6584278345108032\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.656437873840332\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.7504355311393738\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.7434192895889282\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.7383781671524048\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.7337151765823364\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.7296651005744934\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.7262304425239563\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.7235913872718811\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.721241295337677\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.7190194129943848\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.7168445587158203\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.7152668237686157\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.7137113809585571\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.7122131586074829\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.7108045220375061\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.7094849944114685\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.7082399129867554\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.7070164680480957\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.7056456804275513\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.7043374180793762\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.7030921578407288\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.7021499872207642\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.7011722326278687\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.7002246975898743\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.6993184685707092\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.6984124779701233\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.6975535154342651\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.6966835856437683\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.695796549320221\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.6949540972709656\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.6941823959350586\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.7889652848243713\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.7821363806724548\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.7775098085403442\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.7740707397460938\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.771251380443573\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.7686482071876526\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.7663257122039795\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.7643006443977356\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.7624475955963135\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.760739266872406\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.759530782699585\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.7583302855491638\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.7571252584457397\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.7560139894485474\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.7549888491630554\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.7538962960243225\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.7527745962142944\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.7514723539352417\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.7503253221511841\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.7494198083877563\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.7487487196922302\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.7480814456939697\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.7474099397659302\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.7467711567878723\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.7461405396461487\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.7454991340637207\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.7448689937591553\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.744226336479187\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.7435583472251892\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.7428767681121826\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.7157115936279297\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.7107948064804077\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.7071503400802612\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.7040773034095764\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.701673686504364\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.6997819542884827\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.6982074975967407\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.6969910264015198\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.6958563327789307\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.6947537660598755\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.6939128637313843\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.6930992007255554\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.6923157572746277\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.6915620565414429\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.6908447742462158\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.6900479793548584\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.6892585754394531\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.6884335279464722\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.687515139579773\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.6867344379425049\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.6862054467201233\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.6856859922409058\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.6851931810379028\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.6847391724586487\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.6842825412750244\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.6838351488113403\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.6834244728088379\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.6830155849456787\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.6826035976409912\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.6821752786636353\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.5985374450683594\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.5924858450889587\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.587847888469696\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.5841911435127258\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.5810671448707581\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.5782817602157593\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.5755495429039001\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.5728726387023926\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.5706039667129517\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.5685655474662781\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.5669938921928406\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.5654974579811096\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.5641278028488159\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.5629264116287231\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.5618550777435303\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.5608739256858826\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.5601202845573425\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.5594145059585571\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.5587097406387329\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.5579938888549805\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.5574937462806702\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.5570427775382996\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.5566300749778748\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.5562411546707153\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.5558522343635559\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.5554596185684204\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.5550762414932251\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.5547003149986267\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.5543393492698669\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.5539783835411072\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.7345041036605835\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.7276068329811096\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.7218945026397705\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.7172502279281616\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.7128127217292786\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.7082740068435669\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.7038484215736389\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.6998988389968872\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.6959043145179749\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.6928439140319824\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.6908232569694519\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.6891852021217346\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.6876519322395325\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.6861589550971985\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.6845671534538269\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.6830599904060364\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.6815564632415771\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.6800985932350159\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.6785586476325989\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.6768670082092285\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.6753795146942139\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.673866331577301\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.6723687648773193\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.6709784269332886\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.6695368885993958\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.6681447625160217\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.6669256091117859\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.6657201647758484\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.6645609736442566\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.663439929485321\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.7111642956733704\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.708005964756012\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.70538729429245\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.7029600143432617\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.7008394002914429\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.6989200711250305\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.6972774863243103\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.6959004402160645\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.6946983337402344\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.6935184597969055\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.6926733255386353\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.6918277144432068\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.6910162568092346\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.6902894973754883\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.6895811557769775\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.6888960599899292\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.6881672143936157\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.687441885471344\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.686725378036499\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.6860596537590027\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.6855102777481079\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.6849672794342041\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.6844555735588074\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.683966338634491\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.6834535002708435\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.6829361915588379\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.6824529767036438\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.6819988489151001\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.681553840637207\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.6811215281486511\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.5657867789268494\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.5601399540901184\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.5558177828788757\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.5526062846183777\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.5501695275306702\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.548314094543457\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.5467134714126587\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.5454997420310974\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.5444960594177246\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.5435827374458313\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.5429348349571228\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.542317271232605\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.5417039394378662\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.5411622524261475\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.5406634211540222\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.5402209162712097\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.5398203134536743\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.5394428968429565\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.5390706062316895\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.5386990904808044\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.5384073853492737\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.5381239056587219\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.5378372669219971\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.537559986114502\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.5373108983039856\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.5370607972145081\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.5368250012397766\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.5365989804267883\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.536378800868988\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.5361611843109131\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.6323720216751099\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.6290217638015747\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.6266050338745117\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.6245300769805908\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.6224787831306458\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.620813250541687\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.6193388104438782\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.618019163608551\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.6167518496513367\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.6156541705131531\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.6147494912147522\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.6139398813247681\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.6131331324577332\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.6122846603393555\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.6114826798439026\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.6106827259063721\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.609923243522644\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.6092944145202637\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.60859614610672\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.6079970002174377\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.6075222492218018\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.6069466471672058\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.6063283085823059\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.6057553887367249\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.6051281094551086\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.6045462489128113\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.6040105223655701\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.6034837961196899\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.6029709577560425\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.6025182604789734\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.7410002946853638\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.7369638085365295\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.7341024279594421\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.7317764163017273\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.7298383712768555\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.7281625270843506\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.7267543077468872\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.7254037857055664\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.7242118716239929\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.7231464982032776\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.7223678231239319\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.7216554880142212\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.720991849899292\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.7203092575073242\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.719610333442688\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.7188112139701843\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.7180038094520569\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.7172855734825134\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.7165732979774475\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.715819239616394\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.7152414321899414\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.7147169709205627\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.7142094373703003\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.713706374168396\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.7132430672645569\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.7128432393074036\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.7124751806259155\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.7121003270149231\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.7117276191711426\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.7113672494888306\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.7560126781463623\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.7499281167984009\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.7463230490684509\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.7436289191246033\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.7415785789489746\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.7398876547813416\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.7384814023971558\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.7372961640357971\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.7362932562828064\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.7353985905647278\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.7348229885101318\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.7342915534973145\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.733773410320282\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.7332019805908203\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.7326191663742065\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.732048749923706\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.7314764261245728\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.7307937741279602\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.7301066517829895\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.7294273376464844\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.7289758920669556\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.7285504937171936\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.7280955910682678\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.7276563048362732\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.7272235155105591\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.7268284559249878\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.7264517545700073\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.7260792255401611\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.7257332801818848\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.7253847718238831\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.6752666234970093\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.6644765138626099\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.657890260219574\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.653367817401886\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.6503533124923706\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.6482765674591064\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.64682936668396\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.6456056237220764\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.6444204449653625\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.6432955861091614\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.6423758268356323\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.6414546966552734\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.6405845284461975\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.6396740674972534\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.6388962864875793\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.6381730437278748\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.6374664902687073\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.6367465257644653\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.6361086964607239\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.6355185508728027\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.6350657343864441\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.6346530318260193\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.63427734375\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.6339048147201538\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.6335443258285522\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.633213996887207\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.6328948736190796\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.6325988173484802\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.6323150396347046\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.6320804953575134\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.682852566242218\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.6750022768974304\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.6693916320800781\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.6651865839958191\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.6621679663658142\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.6598404049873352\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.6580277681350708\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.6564455628395081\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.6550033092498779\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.6537168025970459\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.6526797413825989\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.6517260074615479\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.6508573889732361\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.6500434279441833\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.6492694616317749\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.6485763788223267\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.6479401588439941\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.6473366022109985\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.6467263102531433\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.646094560623169\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.6456159949302673\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.6451340913772583\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.6446927785873413\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.6443144083023071\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.6439563632011414\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.6436090469360352\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.643259584903717\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.6429073214530945\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.6425873637199402\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.6422948241233826\n",
      " iter:    99 | LR: 0.0100 | Loss Val: 0.5520797967910767\n",
      " iter:   199 | LR: 0.0100 | Loss Val: 0.5437088012695312\n",
      " iter:   299 | LR: 0.0100 | Loss Val: 0.5395205616950989\n",
      " iter:   399 | LR: 0.0100 | Loss Val: 0.5366173982620239\n",
      " iter:   499 | LR: 0.0100 | Loss Val: 0.5344737768173218\n",
      " iter:   599 | LR: 0.0100 | Loss Val: 0.5329316854476929\n",
      " iter:   699 | LR: 0.0100 | Loss Val: 0.5316219925880432\n",
      " iter:   799 | LR: 0.0100 | Loss Val: 0.5305283069610596\n",
      " iter:   899 | LR: 0.0100 | Loss Val: 0.5295329093933105\n",
      " iter:   999 | LR: 0.0080 | Loss Val: 0.5286263823509216\n",
      " iter:  1099 | LR: 0.0080 | Loss Val: 0.5279271006584167\n",
      " iter:  1199 | LR: 0.0080 | Loss Val: 0.5272315740585327\n",
      " iter:  1299 | LR: 0.0080 | Loss Val: 0.5265849828720093\n",
      " iter:  1399 | LR: 0.0080 | Loss Val: 0.5259534120559692\n",
      " iter:  1499 | LR: 0.0080 | Loss Val: 0.525327205657959\n",
      " iter:  1599 | LR: 0.0080 | Loss Val: 0.524733304977417\n",
      " iter:  1699 | LR: 0.0080 | Loss Val: 0.5241773128509521\n",
      " iter:  1799 | LR: 0.0080 | Loss Val: 0.5236481428146362\n",
      " iter:  1899 | LR: 0.0080 | Loss Val: 0.5231029987335205\n",
      " iter:  1999 | LR: 0.0064 | Loss Val: 0.522546112537384\n",
      " iter:  2099 | LR: 0.0064 | Loss Val: 0.522093653678894\n",
      " iter:  2199 | LR: 0.0064 | Loss Val: 0.52166348695755\n",
      " iter:  2299 | LR: 0.0064 | Loss Val: 0.5212679505348206\n",
      " iter:  2399 | LR: 0.0064 | Loss Val: 0.5209237933158875\n",
      " iter:  2499 | LR: 0.0064 | Loss Val: 0.5205845236778259\n",
      " iter:  2599 | LR: 0.0064 | Loss Val: 0.5202489495277405\n",
      " iter:  2699 | LR: 0.0064 | Loss Val: 0.5199195146560669\n",
      " iter:  2799 | LR: 0.0064 | Loss Val: 0.5196089744567871\n",
      " iter:  2899 | LR: 0.0064 | Loss Val: 0.5192993879318237\n",
      " iter:  2999 | LR: 0.0051 | Loss Val: 0.5189979672431946\n"
     ]
    }
   ],
   "source": [
    "innocent_set = custom_dataset('cifar10', [0, 1, 2, 3, 4], 0)\n",
    "# innocent_loader= torch.utils.data.DataLoader(innocent_set, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "#target 后门攻击中的target\n",
    "target_label = 0\n",
    "# target_set = LabeledDataset('cifar', f\"/home/mhc/public_dataset/cifar_imgs/train/{target_label}\", target_label, (1, 2561), cifar_trans)\n",
    "# target_loader = torch.utils.data.DataLoader(target_set, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "rec_label=7  # 提前设置好要误导的类别  注意同步修改模型\n",
    "rec_set = LabeledDataset('cifar', f\"./image/poison_img/exp_50/original\", \n",
    "                            target_label, (1, 2817), transform=transforms.ToTensor())\n",
    "\n",
    "# rec_set = LabeledDataset('cifar', f\"/home/mhc/public_dataset/cifar_imgs/train/{rec_label}\", \n",
    "#                             target_label, (1, 2561), transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "\n",
    "# rec_loader = torch.utils.data.DataLoader(rec_set, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "composite_ds = torch.utils.data.ConcatDataset((innocent_set, rec_set))\n",
    "composite_loader = torch.utils.data.DataLoader(composite_ds, batch_size=256, shuffle=True, num_workers=2)\n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "def trigger_gen(net, dl):\n",
    "\n",
    "    tri = torch.randn(3, 8, 8, requires_grad=True)\n",
    "    tri[:,:,:] = 0.5\n",
    "\n",
    "    iter_source = iter(dl)\n",
    "    for i in range(len(dl)): \n",
    "        data = next(iter_source)\n",
    "        \n",
    "\n",
    "        tri = pgd(net, data, tri)\n",
    "\n",
    "        fname = '/home/mhc/AIJack/invert_and_poison/image/triggers/specific/iter{}.jpg'.format(i)\n",
    "        if not os.path.exists(os.path.dirname(fname)):\n",
    "            os.makedirs(os.path.dirname(fname))\n",
    "            \n",
    "        save_img(tri, fname)\n",
    "\n",
    "\n",
    "trigger_gen(model, composite_loader)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.7675350701402806\n"
     ]
    }
   ],
   "source": [
    "def trigger_test_total(n, loader, trigger, size=8):\n",
    "    n.eval()\n",
    "    total =0\n",
    "    correct =0\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "                \n",
    "        imgs[:, :, 22:30, 22:30] = trigger\n",
    "\n",
    "        output, _ = n(imgs)\n",
    "        \n",
    "        _, preds = torch.max(output.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    print(\"Acc:\",correct/total)\n",
    "\n",
    "\n",
    "triggertrans = transforms.Compose([\n",
    "transforms.Resize((8,8)),\n",
    "transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trigger = Image.open('./image/triggers/specific/trigger13/iter14.jpg').convert('RGB')\n",
    "trigger = triggertrans(trigger) # size [3, 8, 8]\n",
    "\n",
    "test_img = 7\n",
    "test_label = 2\n",
    "test_set = LabeledDataset('cifar', f\"/home/mhc/public_dataset/cifar_imgs/train/{test_img}\", test_label, (1, 500), transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "model = alexnet(\"cifar10\",False)\n",
    "model.load_state_dict(torch.load(\"/home/mhc/AIJack/invert_and_poison/checkpoint/experiment_54/globmod/epoch_24_acc_0.7.pth\")[\"state_dict\"])\n",
    "\n",
    "trigger_test_total(model, test_loader, trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "patch_size = 8\n",
    "lr = 0.0001\n",
    "eps = (255/255.0)\n",
    "batch_size = 256\n",
    "num_iter = 3000  # PGD iteration\n",
    "\n",
    "losses = AverageMeter()\n",
    "cossim = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "iter_source = iter(rec_loader)\n",
    "iter_target = iter(target_loader)\n",
    "\n",
    "tri = torch.randn(3, patch_size, patch_size, requires_grad=True)\n",
    "tri[:,:,:] = 0.5\n",
    "tri = nn.Parameter(tri.to(device))\n",
    "\n",
    "for i in range(len(target_loader)): \n",
    "\t# LOAD ONE BATCH OF SOURCE AND ONE BATCH OF TARGET ITERALLY\n",
    "\t(input1, label1) = next(iter_target)\n",
    "\t(input2, label2) = next(iter_source)\n",
    "\t\n",
    "\n",
    "\tinput1 = input1.to(device)\n",
    "\tinput2 = input2.to(device)\n",
    "\t\n",
    "\toutput1, feat1 = model(input1)\n",
    "\t# feat1 = feat1.detach().clone()\n",
    "\t# print(\"feat1 size:\", feat1.size()) [B, fc_in] [256,2304]\n",
    "\n",
    "\n",
    "\t# PGD iteration \n",
    "\tfor j in range(num_iter):\n",
    "\n",
    "\t\tif tri.grad == None:\n",
    "\t\t\ttri = nn.Parameter(tri.to(device))\n",
    "\n",
    "\t\telse:\n",
    "\t\t\ttri.grad.data.zero_()\n",
    "\n",
    "\t\tlr1 = adjust_learning_rate(lr, j+1)\n",
    "\n",
    "\t\tinput2[:, :, 32-2-patch_size:32-2, 32-2-patch_size:32-2] = tri\n",
    "\t\toutput2, feat2 = model(input2)\n",
    "\n",
    "\t\t# FIND CLOSEST PAIR WITHOUT REPLACEMENT using greedy alorithm\n",
    "\t\t# feat11 = feat1.clone()\n",
    "\t\t# dist = torch.cdist(feat1, feat2)\n",
    "\t\t# # print(\"dist size:\", dist.size()) [B, B]\n",
    "\t\t\n",
    "\t\t# for _ in range(feat2.size(0)):\n",
    "\t\t# \tdist_min_index = (dist == torch.min(dist)).nonzero(as_tuple=False).squeeze()\n",
    "\t\t# \tfeat1[dist_min_index[1]] = feat11[dist_min_index[0]]\n",
    "\t\t# \tdist[dist_min_index[0], dist_min_index[1]] = 1e5\n",
    "\n",
    "\t\t# MSE  cosine_similarity ? \n",
    "\t\tloss1 = ((feat1-feat2)**2).sum(dim=1)\n",
    "\t\t# loss1 = - cossim(feat1, feat2)\n",
    "\t\tloss = loss1.sum()\n",
    "\n",
    "\t\tlosses.update(loss.item(), input1.size(0))\n",
    "\n",
    "\t\tloss.backward(retain_graph=True)\n",
    "\n",
    "\t\ttrigrad = tri.grad\n",
    "\t\ttri = tri - lr1*tri.grad\n",
    "\t\ttri = torch.clamp(tri, 0, eps).detach_()\n",
    "\n",
    "\t\t# input2_origin = input2.detach().clone()\n",
    "\t\t# input2[:, :, 32-2-patch_size:32-2, 32-2-patch_size:32-2] = tri\n",
    "\t\t# input2 = input2.clamp(0, 1)\n",
    "\n",
    "\t\tif (j+1) %50 == 0:\n",
    "\t\t\tprint(\" i: {} | iter: {:5d} | LR: {:2.4f} | Loss Val: {:5.3f} | Loss Avg: {:5.3f}\"\n",
    "\t\t\t\t\t\t.format( i, j, lr1, losses.val, losses.avg))\n",
    "\t\t\n",
    "\t\t# end the optimization \n",
    "\t\tif j == (num_iter-1):\n",
    "\t\t\tfname = '/home/mhc/AIJack/invert_and_poison/image/triggers/specific/iter{}.jpg'.format(i)\n",
    "\t\t\tif not os.path.exists(os.path.dirname(fname)):\n",
    "\t\t\t\tos.makedirs(os.path.dirname(fname))\n",
    "\t\t\t\t\n",
    "\t\t\tsave_img(tri, fname)\n",
    "\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\t# tri = (input2 - input2_origin)[0, :, 32-2-patch_size:32-2, 32-2-patch_size:32-2]  # [256, 1, 32, 32] -> [3,8,8], 且各个维度相同\n",
    "\t\t\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ee93600a6eb60504e508f539cf2b837386ccee6f718dcf413180ced79f68df2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
